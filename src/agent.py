import numpy as np
import torch
import torch.nn as nn
from gymnasium.vector.vector_env import VectorEnv
from .task_selectors.ins.negation_layer import NegationMask
from torch.distributions.normal import Normal


def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class Agent(nn.Module):
    def __init__(self, envs: VectorEnv):
        super().__init__()

        features = np.array(envs.single_observation_space.shape).prod()
        actions = np.prod(envs.single_action_space.shape)

        self.critic = nn.Sequential(
            layer_init(nn.Linear(features, 64)),
            # NegationMask(64),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            # NegationMask(64),
            nn.Tanh(),
            layer_init(nn.Linear(64, 1), std=1.0),
        )
        self.actor_mean = nn.Sequential(
            layer_init(nn.Linear(features, 64)),
            # NegationMask(64),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            # NegationMask(64),
            nn.Tanh(),
            layer_init(nn.Linear(64, actions), std=0.01),
        )
        self.actor_logstd = nn.Parameter(torch.zeros(1, actions))

    def get_value(self, x: torch.Tensor) -> torch.Tensor:
        return self.critic(x)

    def get_action_and_value(
        self, x: torch.Tensor, action: torch.Tensor | None = None
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        action_mean = self.actor_mean(x)
        action_logstd = self.actor_logstd.expand_as(action_mean)
        action_std = torch.exp(action_logstd)
        probs = Normal(action_mean, action_std)
        if action is None:
            action = probs.sample()
        return (
            action,
            probs.log_prob(action).sum(1),
            probs.entropy().sum(1),
            self.critic(x),
        )
